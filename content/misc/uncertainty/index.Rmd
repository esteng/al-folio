---
title: Annotator Instructions
author: Elias Stengel-Eskin
summary: Instructions for Annotating Uncertainty HIT
date: 2021-12-01
math: true
diagram: true
categories:
  - misc
tags:
  - misc
projects:
  - none
image:
  placement: 3
  caption: 
featured: no
---

## Table of Contents

1. [Introduction](#introduction)
    a. [About the interface](#about-the-interface)
2. [Description of Categories](#description-of-categories)
    a. [Low Quality Image](#low-quality-image) 
    b. [Invalid Question/Spam Answer](#invalid-question-or-spam-answer) 
    c. [Difficult/Time Consuming](#difficult-or-time-consuming) 
    d. [Synonyms/Hypernyms](#synonyms-or-hypernyms) 
    e. [Answer Not Present/Guessing](#answer-not-present-or-guessing) 
    f. [Ambiguous](#ambiguous) 
    g. [Choice of Many](#choice-of-many) 
    h. [Disagreements of Meaning](#disagreements-of-meaning) 

## Introduction 
For the follow question about the image, the answers provided may differ. Think of each answer as coming from a different person. Some people might disagree about the answers to the question. 

You will see an image and a question about the image, along with a set of responses from different people. All the responses differ (at least superficially). In some cases, the difference is just a matter of different spellings of the same word or concept. However, in other cases, the disagreement is more pronounced (e.g. half of the annotators said "yes", the other half said "no"). 
Below the image and question, there is a set of labels that identify reasons why annotators may have disagreed. **Please note that you're not being asked to answer the question itself, or to decide which answer was correct**. Instead, we would like you to try to infer the reason **why** people disagreed. 

In this document, descriptions and examples are given for each of these labels. 
**Please label with image-question-answer set with the labels, or, if none apply, select "Other" and provide a brief explanation in the text box.** 

### About the interface 
The interface shows the image on the top left, the question and answers on the top right, and the choices on the bottom. 
There is an interface on the right side of the image which allows you to zoom in on the image if needed (you are unlikely to need this). 
Some choices trigger follow-up questions. These will appear to the right of the existing choices. 
With a long image, the choices may be at the bottom of the screen, initially out of view. Please make sure to scroll down to look at all the options before annotating the example. 
Each choice has a superscript digit or letter next to it; by pressing that key on your keyboard, you can toggle the choice on or off. Pressing the Control + Enter key combo will submit the task. These shortcuts can help speed up the task. 
Note that for a given example, multiple choices can be selected, as in some cases examples will span multiple labels. 
**Please try to label each example with as few labels as possible.** Ideally, each of the examples should receive only one label. We have included the option to annotate with multiple labels, but please use this feature sparingly. 

## Description of Categories

### Low Quality Image 
Some images are low quality: for example, they may be blurry or dark. This can lead to annotator disagreement, since the annotators might see the image differently. 

#### Example: 

- Question: What are the girls holding? 
- Answers: "board", "donut", "laptop", "unknown", "tablet", "keyboard", "computer", "laptop computer", "laptop", "laptop"
- Image: ![first image](./images/COCO_train2014_000000444945.jpg) 


### Invalid Question or Spam answer 
This category encompasses two possibilities: invalid questions and spam answers. 
Both the questions and answers were generated by annotators, who sometimes produce nonsense. 
Some questions are unrelated to the image or generally unanswerable. 
Other times, annotators provide nonsense answers to valid questions; this may be deliberate, or maybe they did not understand the question. 
We treat these the same way. 

#### Example (Invalid Question): 

- Question: what are
- Answers: "lights", "lamp", "lamps", "pillows", "hotel"
- Image: ![second image](./images/COCO_train2014_000000109928.jpg) 

#### Example (Spam Answer): 

- Question: Is the man reading?
- Answers: "yes", "yes", "yes", "**magazine**" 
- Image: ![spam answer](./images/COCO_train2014_000000441544.jpg) 

### Difficult or Time Consuming

Questions can be impossible, very difficult, or time-consuming to answer. For example, the question may ask the annotator to count many small objects, making it very hard to get an exact answer. 

#### Example: 

- Question: How many cars are there?
- Answers: "2", "7", "2", "1", "1", "6", "6", "3", "many", "4"
- Image: ![difficult](./images/COCO_train2014_000000445113.jpg) 

### Synonyms or Hypernyms 

Annotators might differ slightly in how they phrase an answer, or identify objects at different levels of granularity. Crucially, for this label, all the annotations should refer to the **same object or concept**. 

#### Example (synonyms):

- Question: What is the person holding?
- Answers: "tennis racket", "tennis racket", "tennis racket", "racquet", "racket", "tennis racket", "tennis racket", "racquet", "racket", "tennis racket"
- Image: ![synonyms](./images/COCO_train2014_000000442961.jpg) 

#### Example (hypernyms):

- Question: What sits on the left hand side of the bowl?
- Answers: "container", "jug", "container", "bottle" 
- Image: ![hypernyms](./images/COCO_train2014_000000438671.jpg) 

### Answer not Present or Guessing 

Questions sometimes are asking for information not visible in the image (e.g. questions about objects partially out of the frame, or not depictable in an image). Annotators will often differ in their answers for such images. 
Clicking this option will open another annotation menu to the right, with the following labels: 

#### Example (motion):

The question asks about the motion of an object; since we're dealing with images, this can be hard to infer. 

- Question: Is the train moving?
- Answers:  "no", "no", "no", "no", "yes", "no", "yes", "yes", "yes", "yes"
- Image: ![motion](./images/COCO_train2014_000000441034.jpg) 

#### Example (out of view):

The question asks about an object or event that is out of view of the image 

- Question: What is inside the yellow plastic object?
- Answers: "groceries", "stuff", "products", "toiletries", "stuff", "food", "food", "luggage", 
- Image: ![out-of-view](./images/COCO_train2014_000000445462.jpg) 

#### Example (predicting the future):

The question asks about whether something will happen in the future. 

- Question: Is the woman going to eat it all?
- Answers:  "no", "yes", "no", "no", "no", "no", "no", "yes", "no"
- Image: ![predicting future](./images/COCO_train2014_000000439576.jpg) 

#### Example (hypothetical): 

The question asks whether something hypothetically would happen. 

- Question: Could passenger be boarding?
- Answers: "yes", "no", "no", "yes", "yes", "no" 
- Image: ![hypothetical](./images/COCO_train2014_000000444210.jpg) 

#### Example (domain knowledge): 

The question might be any one of the above (motion, out of view, prediciting the future, hypothetical), but the answers indicate that some people give a certain answer based on domain knowledge, while others might give a different, less informed answer. This happens often with sports. 

- Question: What trick is he performing?
- Answers:  "rail riding", "ollie", "skateboard trick", "rail grinding", "railslide", "rail slide", "grinding", "slide", "loop", "sliding on rail"
- Image: ![domain](./images/COCO_train2014_000000044327.jpg)  

There is also an "other" option which will prompt you to provide your own answer. 

### Ambiguous

This category is very broad, and may overlap with other categories. At the end of this instruction document are some examples to help distinguish ambiguous from other related categories. 
In some cases, the image is ambiguous; in other cases, it is the question. It is also possible for both to be ambiguous. 


#### Example (image) 

This option should be used when an image is visually ambiguous.

- Question: How many lamps are on the counter ?
- Answers: "1", "1", "2", "1", "2", "2", "1" 
- Image: ![visually-ambiguous](./images/COCO_train2014_000000465213.jpg) 


#### Example (question)

This option should be used when a question is linguistically ambiguous.
**This includes questions where annotators could take a different perspective to get different answers (e.g. left vs right)**

- Question: In which direction is the wave moving?
- Answers:  "south", "forward", "right", "forward", "down", "right", "toward beach", "right", "left to right", "left"
- Image: ![blah](./images/COCO_train2014_000000440314.jpg) 

### Choice of many

When annotators are asked to choose one thing from an image and there are many options, they will often choose different ones. This happens frequently with text or colors.

#### Example (text) 

- Question: What does the boy's shirt say?
- Answers: "eagles", "miami eagles legends, all state champions usa", "america beautiful", "miami eagles usa", "m", "miami eagles", "miami eagles"
- Image: ![choose text](./images/COCO_train2014_000000443082.jpg) 

#### Example (color) 

- Question: What color are the boats?
- Answers:  "blue", "blue, white", "blue, brown", "blue and beige", "blue green and yellow", "blue and white", "blue, white", "blue and yellow", "blue and yellow", "blue"
- Image: ![choose color](./images/COCO_train2014_000000443650.jpg)

### Subjective

Some questions ask for subjective assessments (e.g. whether people are happy, sad, having fun, whether a painting is beautiful, etc.). 

#### Example

- Question: Is he depressed?
- Answers:  "no", "yes", "no", "no", "no", "yes", "no", "yes", "yes", "yes"
- Image: ![subjective](./images/COCO_train2014_000000439550.jpg) 

### Disagreements of meaning

Sometimes you can pinpoint the exact word that annotators seem to disagree on. If you select this option, you'll be asked to provide a word or a few words from the question that annotators seem to disagree on. 
For example, in the following example, some annotators consider humans to be animals, while others don't. So for this example, you'd put "animal" into the text box after clicking this option. 

#### Example

- Question: What animal is on the flatbed?
- Answers: "none", "none", "human", "none", "no", "man", "people", "no animal", "none",
- Image: ![disagree](./images/COCO_train2014_000000443909.jpg)


## Potential conflicts

### Invalid question vs Low quality image

### Ambiguous vs Difficult 
