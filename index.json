[{"authors":["admin"],"categories":null,"content":"I am a second-year Ph.D. student at the Center for Language and Speech Processing supervised by Benjamin Van Durme, where I am working on broad questions in computational semantics. My most recent line of work is on semantic parsing for the Universal Decompositional Semantics formalism. I\u0026rsquo;m also interested in applying ideas from the philosophy of language to NLP and in multimodal semantic parsing from text and images. Before starting my Ph.D., I received my B.A.\u0026amp;Sc. with First Class Honours in Cognitive Science from McGill University, focusing in computer science and linguistics. While at McGill, I worked as a research assistant at the Montreal Language Modeling Lab (MLML), now MCQLL. I wrote my honours thesis (supervised by Timothy O\u0026rsquo;Donnell) on a variational inference algorithm for a model of language acquisition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/elias-stengel-eskin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/elias-stengel-eskin/","section":"authors","summary":"I am a second-year Ph.D. student at the Center for Language and Speech Processing supervised by Benjamin Van Durme, where I am working on broad questions in computational semantics. My most recent line of work is on semantic parsing for the Universal Decompositional Semantics formalism.","tags":null,"title":"Elias Stengel-Eskin","type":"authors"},{"authors":[],"categories":null,"content":"","date":1594213200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594213200,"objectID":"b56bb7ceb92d2fdb402b4c8c5bd8a45a","permalink":"/talk/2020-acl/","publishdate":"2020-06-20T00:00:00Z","relpermalink":"/talk/2020-acl/","section":"talk","summary":"We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS) representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attribute scores. We also introduce a strong pipeline model for parsing into the UDS graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction. By analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups.","tags":["teaching"],"title":"Universal Decompositional Semantic Parsing","type":"talk"},{"authors":null,"categories":["decomp","linguistics"],"content":" Consider the following two scenarios:\nDerek was over for dinner at Lisa's house. After dinner, he offered to do the dishes, but as he was picking up one of the plates his grip slipped and he dropped it. Later, when telling her friend about the night, Lisa said, \u0026quot;Derek broke the plate.\u0026quot;\n Derek was over for dinner at Lisa's house. Lisa claimed that she bought special \u0026quot;unbreakable\u0026quot; plates and challenged Derek to break one. He promptly smacked the plate against the edge of the counter, and broke it. Later, when telling her friend about the night, Lisa said, \u0026quot;Derek broke the plate.\u0026quot;\n    1 2         Both times, the statement \u0026quot;Derek broke the plate\u0026quot; remains unchanged. But based on the different scenarios, we as readers draw very different conclusions. Evidence suggests that even toddlers are able to draw inferences about intentionality and volition (Vaish, Carpenter, and Tomasello 2010).1 Inferences like that in (2), Derek meant to break the plate, while in (1) it was an accident. Furthermore, different scenarios might evoke different levels of volition; we could easily imagine a range of set-ups where Derek is more or less volitional, between the two extremes laid out above. We can also draw a much broader range of inferences from these sentences; for example, we know that Derek is sentient while the plate is not, and that the plate physically changed during the process, but Derek probably wasn't. We know that the plate probably doesn't exist anymore, but Derek does. The list goes on...\nThis raises a few key questions: how is it that identical sentences can evoke a range of different inferences? What kinds of inferences do we draw from language? What other hidden knowledge is lying just under the surface of the language we speak?\nA rich array of work has looked at these questions, including Dowty (1991), who introduced a number of inferences, including \u0026quot;volition\u0026quot;, \u0026quot;awareness\u0026quot;, \u0026quot;sentience\u0026quot;, \u0026quot;change of state\u0026quot;, and \u0026quot;existed before/during/after\u0026quot;. From there, D. Reisinger et al. (2015), White et al. (2016), Govindarajan, Van Durme, and White (2019), and Vashishtha, Van Durme, and White (2019) annotated a corpus of English sentences these properties (among others) and began modeling them. I myself joined this effort fairly late, contributing to White et al. (2019), which consolidated annotations from across these works into one dataset, the Universal Decompositional Semantics Dataset, or UDSv1.0. My main contribution to this line of work has been working on a parsing model, which learns to automatically extract these inferences from text (Stengel-Eskin et al. 2020).\nBy way of starting, I'll try to summarize the different types of annotations in the dataset. The dataset gives each property a score from -3 to 3, with 3 being \u0026quot;very likely\u0026quot; and -3 being \u0026quot;very unlikely.\u0026quot; The words the annotation applies to are given in bold.\n    Annotation Description Examples    Factuality Factuality inferences represent how likely (or unlikely) a listener thinks a scenario that is to have occurred. Jo left (3), Jo didn’t leave (-3), Jo thought that Cole had left (-1)  Genericity Genericity refers to inferences about the generality of events or event participants Ex. property: genericity-predicate-particular: Amy ate oats for breakfast today (3), Amy ate oats for breakfast every day (-3)  Time Temporal inferences are inferences about how long events might take Ex. property: time-duration-minutes: Tom left (-3), Tom was singing (3)  Word Sense Words can have different senses/supersenses (e.g. bank as a place to keep money (phyiscal location) vs. the bank of a river, (geographical feature)). These sense can apply to differing degrees. Ex. property: \u0026quot;person\u0026quot; supersense Sandy led Rufus by a leash (-3), Sandy led Rufus by a leash (3)  Semantic proto-roles SPR properties (introduced by Dowty (1991)) capture properties typically associated with either agents (the person/thing doing the acting in a sitation) and patients (the person/thing that's being acted upon), like awareness and volition (agent properties) or change of state and being created/destroyed (patient properties) Ex. property: volition Derek broke his arm (-3), Derek broke the wishbone (3)  Ex. property: was used The fragile vase was shipped with bubble-wrap (3), The fragile vase was shipped with haste (-3)    Excavation via other languages When we look at these questions from one linguistic lens alone, we get an impoverished view. Some inferences will present themselves readily, while others will be purely contextual. For example from the English point of view, information on when an event happened is often presented explicitly in the tense of the verb; in another language (such as Mandarin) this information might need to be inferred from context, much like how Derek's level volition in breaking the plate in English needs to be inferred.\nOther languages offer different lenses through which to view these semantic inferences by varying what they make explicit and what is kept below the surface. Think of semantics as a lost city or desert ruin, with different languages acting as different winds, shifting the surface-level sand to uncover different aspects of the same semantic structure.\n Back to Derek and the plate Let's revisit Derek, but this time in a different linguistic setting -- say, Mexico City. Where in English, we'd use the same construction in both scenarios (volitional and non-volitional), in Spanish, we'd probably use a different construction. In scenarios (1) and (2), we'd say something like\n  Spanish Se le rompió el plato a Derek    Direct English gloss Itself to him broke the plate to Derek  Rough English gloss The plate broke itself to Derek  English translation   Derek broke the plate       Spanish Derek rompió el plato    En. gloss Derek broke the plate  En. translation Derek broke the plate     Both can translate to \u0026quot;Derek broke the plate\u0026quot; in English, but the first utterance indicates that it was an accident. A direct gloss of each sentence is immediately below the example. Funnily enough, the direct gloss here looks a lot like what we might find in German!\n  German Der Teller ist dem Derek zerbrochen    En. gloss The-NOM plate-NOM is-AUX the-DAT Derek-DAT broken  Rough En. gloss The plate broke to Derek   En. translation  Derek broke the plate       Derek hat den Teller zerbrochen.    Derek has the-ACC plate-ACC broken  Derek broke the plate 2     So looking at two other languages tells us that while English doesn't necessarily make the distinction between volitional and non-volitional plate breaking (at least in the active voice), other languages do!3\n What about some other inferences Tired of Derek and his plates? Me too. Why not take a step up the linguistic family tree from Spanish and look at its parent, Latin. Latin is a fantastic language for exploring these kinds of inferences for a few reasons. The first is that its a morphologically rich language -- many of the grammatical relations which are encoded by word position in other languages (e.g. subject, predicate, and object in English) are encoded directly on the words. This means that word order is much less important. It also means that wealth of semantic information is encoded in the morphology of a word.\nAnother reason Latin is great for looking at these inferences is the way it's taught. Nouns (and their corresponding adjectives) have cases in Latin, which are different endings to the same stem (much like how verbs in English have different endings for different tenses/moods/persons). The cases are nominative, genitive, dative, accusative, ablative, and vocative (and sometimes locative), all with different uses. To help students remember which cases are used when, the cases come with handy names that are taught in textbooks, like \u0026quot;the ablative of agent\u0026quot; or \u0026quot;the dative of reference.\u0026quot; We can match some of these handy mnemonics directly to certain inferences described in Dowty (1991). For example, the \u0026quot;was used\u0026quot; property fits well with the ablative of instrument, where an ablative ending tells us something was used as a tool in an action, i.e.\n  Caesar gladiis occidus est    Caesar-NOM swords-ABL killed-NOM is-AUX  Caesar with swords killed was  Caesare was killed with swords    Similarly, Dowty's \u0026quot;was for the benefit of\u0026quot; property demands nothing less than the dative, in this case often known as the dative of advantage/disadvantage. Cui bono is a famous example of this.4\nThese mnemonic devices not only help students learn Latin, but also help to expose some of the underlying semantic features of the language.\n Genericity Genericity inferences pertain to how particular or abstract a predicate or argument is. For example, it's clear that \u0026quot;dogs\u0026quot; in \u0026quot;My uncle Roger's dogs Fifi and Fido\u0026quot; is different from \u0026quot;dogs\u0026quot; in \u0026quot;my brother is afraid of dogs\u0026quot;, in that the first \u0026quot;dogs\u0026quot; is very particular, while the second one applies to more of a natural kind. Similarly, predicates can be more or less particular or general. Consider:\nMarie ate oatmeal for breakfast every day. Marie ate oatmeal for breakfast yesterday.  The \u0026quot;ate\u0026quot; in (3) is different from the \u0026quot;ate\u0026quot; in (4), which encompases a broader range of instances of an \u0026quot;eating\u0026quot; event. French makes this distinction quite clear on the surface level:\n  Marie a mangé du gruau pour son petit déjeuner hier    Marie has eaten of the oatmeal for her small lunch yesterday  Marie ate oatmeal for breakfast yesteray         Marie mangeait du gruau pour son petit déjeuner tous les jours    Marie ate-PROG of the oatmeal for her small lunch all the days  Marie ate (imperf.) oatmeal for breakfast every day      Here, we have two different tenses for actions in the past which were repeated or more general, and those which are more specific instances. These same tenses (imperfect and perfect) relate also to time duration annotations, which can be found in the UDS dataset (White et al. 2019).\n Awareness/sentience Awareness and sentience are probably my favorite proto-roles; what could better describe the impressive human capacity for language than the entirely unconscious ability to rapidly draw inferences about philosophically layered concepts like awareness and sentience? German (another morphologically rich language) makes certain inferences about awareness and sentience quite clear in its use of case with certain verbs. In English, a speaker surprised by something might say:\nI don't believe it!  While an incredulous person might say:\nI don't believe him!  where the \u0026quot;it\u0026quot; in (5) is clearly non-sentient (i.e. a news story, a state of affairs, a juicy piece of gossip, etc.) while \u0026quot;him\u0026quot; in (6) is sentient. In German, these two objects, while arguments of the same verb, take different cases (accusative and dative, respectively):\n  Ich glaube es nicht!    I believe it-ACC not  I don't believe it      Ich glaube ihm nicht!    I believe him-DAT not  I don't believe him     A German speaker thus explicitly represents beliefs about the sentience/non-sentience of an object while choosing a case for an object.\n Conclusions By no means am I suggesting, with the examples above, that these inferences are impossible to draw in different languages, or even necessarily more difficult. All languages are infinitely expressive, and given enough clarification, any inferences about proto-roles, genericity, time duration, or word sense can be made as plain as in any other language. I instead hope to draw attention to the ways that languages encode some of these inferences explicitly in the grammar, lending support to their status as real linguistic concepts rather than figments of the imagination.\nNor do I want to give the impression that speakers of these different languages differ in their judgements of these inferences, or that having the inferences explicitly represented in the grammar or morphology somehow increases our awareness of them. I don't know of any research to this effect, and have not experienced this myself. In fact, the miracle is that all of this happens largely unconsciously, and that there seems to be wide agreement between speakers on these inferences. That all speakers of a language can even agree on a meaning for what is said is miraculous, but that they can also to a large extent on what is not said is even more astounding.\nFinally, these are just some examples from some languages that I happen to be familiar with. I'd love to find more examples like these, so if you can think of ones in other languages (especially non-WEIRD languages!) please don't hesitate to reach out! Similarly, if you can think of counter-examples in these languages or others, shoot me a message.\n References Dowty, David. 1991. “Thematic Proto-Roles and Argument Selection.” Language 67 (3). Linguistic Society of America: 547–619.\n Govindarajan, Venkata, Benjamin Van Durme, and Aaron Steven White. 2019. “Decomposing Generalization: Models of Generic, Habitual, and Episodic Statements.” Transactions of the Association for Computational Linguistics 7: 501–17.\n Reisinger, Drew, Rachel Rudinger, Francis Ferraro, Craig Harman, Kyle Rawlins, and Benjamin Van Durme. 2015. “Semantic Proto-Roles.” Transactions of the Association for Computational Linguistics 3: 475–88.\n Stengel-Eskin, Elias, Aaron Steven White, Sheng Zhang, and Benjamin Van Durme. 2020. “Universal Decompositional Semantic Parsing.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 8427–39. Online: Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.acl-main.746.\n Vaish, Amrisha, Malinda Carpenter, and Michael Tomasello. 2010. “Young Children Selectively Avoid Helping People with Harmful Intentions.” Child Development 81 (6). Wiley Online Library: 1661–9.\n Vashishtha, Siddharth, Benjamin Van Durme, and Aaron Steven White. 2019. “Fine-Grained Temporal Relation Extraction.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2906–19. Florence, Italy: Association for Computational Linguistics.\n White, Aaron Steven, Drew Reisinger, Keisuke Sakaguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme. 2016. “Universal Decompositional Semantics on Universal Dependencies.” In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 1713–23. Austin, TX: Association for Computational Linguistics.\n White, Aaron Steven, Elias Stengel-Eskin, Siddharth Vashishtha, Venkata Govindarajan, Dee Ann Reisinger, Tim Vieira, Keisuke Sakaguchi, et al. 2019. “The Universal Decompositional Semantics Dataset and Decomp Toolkit.” ArXiv Preprint ArXiv:1909.13851.\n    based on the finding that toddlers show preferences for helping individuals who are perceived to have caused a harmful action accidentally over those who caused the same action on purpose↩\n Where needed, I've included a linguistic gloss (basically a word-for-word translation), with cases like NOM (nominative), ACC (accusative), DAT (dative), etc., as well as a \u0026quot;rough gloss\u0026quot; which gives a more English-ified word-for-word translation of the original sentence, to be more comprehensible for those who aren't familiar with the case system.↩\n I specify the active voice, since we often use the passive voice to indicate a lack of volition, i.e. \u0026quot;The plate was broken by Derek,\u0026quot; although this isn't quite the same as the Spanish and German examples. We also have verbs that optionally take an object, like \u0026quot;break,\u0026quot; that can be used to express a similar notion, i.e. \u0026quot;The plate broke,\u0026quot; but there is no easy way to simultaneously express Derek's involvement.↩\n cui bono is actually a rare \u0026quot;double dative\u0026quot; composed of a dative of advantage and a dative of reference, literally, \u0026quot;for a benefit to whom?\u0026quot;↩\n   ","date":1592956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592956800,"objectID":"6486abc377ba593e06d802197a9bd9aa","permalink":"/post/2020-decomp-xlingual/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/post/2020-decomp-xlingual/","section":"post","summary":"Exploring fine-grained semantic inferences cross-lingually","tags":["comp-ling","decomp"],"title":"Exploring fine-grained semantic inferences cross-lingually","type":"post"},{"authors":["Elias Stengel-Eskin","Aaron Steven White","Sheng Zhang","Benjamin Van Durme"],"categories":null,"content":"","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591574400,"objectID":"10a2fe10c381d001f87f51fba662d32d","permalink":"/publication/2020-decomp-model/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/publication/2020-decomp-model/","section":"publication","summary":"We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS) representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attribute scores. We also introduce a strong pipeline model for parsing into the UDS graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction. By analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups.","tags":null,"title":"Universal Decompositional Semantic Parsing","type":"publication"},{"authors":["Aaron Steven White","Elias Stengel-Eskin","Siddharth Vashishtha","Venkata Govindarajan","Dee Ann Reisinger","Tim Vieira","Keisuke Sakaguchi","Sheng Zhang","Francis Ferraro","Rachel Rudinger","Kyle Rawlins","Benjamin Van Durme"],"categories":null,"content":"","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"5896768c5f4965ecbc7cc74d6624bb13","permalink":"/publication/2020-decomp-dataset/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/publication/2020-decomp-dataset/","section":"publication","summary":"We present the Universal Decompositional Semantics (UDS) dataset (v1. 0), which is bundled with the Decomp toolkit (v0. 1). UDS1. 0 unifies five high-quality, decompositional semantics-aligned annotation sets within a single semantic graph specification---with graph structures defined by the predicative patterns produced by the PredPatt tool and real-valued node and edge attributes constructed using sophisticated normalization procedures. The Decomp toolkit provides a suite of Python 3 tools for querying UDS graphs using SPARQL. Both UDS1. 0 and Decomp0. 1 are publicly available at this http URL.","tags":null,"title":"The Universal Decompositional Semantics Dataset and Decomp Toolkit","type":"publication"},{"authors":[],"categories":null,"content":"","date":1574121600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574121600,"objectID":"c9fffffcef77e9637c2a35fddeca5f3f","permalink":"/talk/2019-lectures/","publishdate":"2019-11-19T00:00:00Z","relpermalink":"/talk/2019-lectures/","section":"talk","summary":"Gave a pair of lectures on DNNs in EN.601.464 (Artificial Intelligence)","tags":["teaching"],"title":"Deep Learning Fundamentals","type":"talk"},{"authors":["Elias Stengel-Eskin","Tzu-Ray Su","Matt Post","Benjamin Van Durme"],"categories":null,"content":"","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"70e9b9dc799160ee1d1a6b426cbe8db4","permalink":"/publication/2019-alignment/","publishdate":"2019-11-03T00:00:00Z","relpermalink":"/publication/2019-alignment/","section":"publication","summary":"We introduce a novel discriminative word alignment model, which we integrate into a Transformer-based machine translation model. In experiments based on a small number of labeled examples (~ 1.7 K-5K sentences) we evaluate its performance intrinsically on both English-Chinese and English-Arabic alignment, where we achieve major improvements over unsupervised baselines (11-27 F1). We evaluate the model extrinsically on data projection for Chinese NER, showing that our alignments lead to higher performance when used to project NER tags from English to Chinese. Finally, we perform an ablation analysis and an annotation experiment that jointly support the utility and feasibility of future manual alignment elicitation.","tags":null,"title":"A Discriminative Neural Model for Cross-Lingual Word Alignment","type":"publication"},{"authors":[],"categories":null,"content":"","date":1571961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571961600,"objectID":"1f1929a8996a956a5704a27101302f7e","permalink":"/talk/2019-phil/","publishdate":"2019-10-25T00:00:00Z","relpermalink":"/talk/2019-phil/","section":"talk","summary":"Summarizes major questions in the philosophy of language and how they relate to various problems in NLP.","tags":["techo","phil-of-lang"],"title":"Philosophy of Language for NLP","type":"talk"},{"authors":[],"categories":null,"content":"","date":1556236800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556236800,"objectID":"bd0a8cc386b8b8887489dd170ccff37f","permalink":"/talk/2019-eval/","publishdate":"2019-04-26T00:00:00Z","relpermalink":"/talk/2019-eval/","section":"talk","summary":"Presented a summary of multiple forms of semantic evaluation for MT to the Textual Choreography reading group.","tags":["techo"],"title":"Semantic Eval for MT","type":"talk"},{"authors":[],"categories":null,"content":"","date":1541721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541721600,"objectID":"b5d8f4d66d659f600e984332dd67668f","permalink":"/talk/2018-alignment/","publishdate":"2018-11-09T00:00:00Z","relpermalink":"/talk/2018-alignment/","section":"talk","summary":"Presented an overview of Ghader and Monz (2017) to the Textual Choreography reading group at JHU.","tags":["techo"],"title":"Attention and Alignment","type":"talk"},{"authors":["Michael McAuliffe","Elias Stengel-Eskin","Michaela Socolof","Morgan Sonderegger"],"categories":null,"content":"","date":1503532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503532800,"objectID":"d9148cfbf868c9a4fc8ed77a85d18167","permalink":"/publication/2017-polyglot/","publishdate":"2017-08-24T00:00:00Z","relpermalink":"/publication/2017-polyglot/","section":"publication","summary":"Speech datasets from many languages, styles, and sources exist in the world, representing significant potential for scientific studies of speech—particularly given structural similarities among all speech datasets. However, studies using multiple speech corpora remain difficult in practice, due to corpus size, complexity, and differing formats. We introduce open-source software for unified corpus analysis: integrating speech corpora and querying across them. Corpora are stored in a custom ‘polyglot persistence’scheme that combines three sub-databases mirroring different data types: a Neo4j graph database to represent temporal annotation graph structure, and SQL and InfluxDB databases to represent meta-and acoustic data. This scheme abstracts away from the idiosyncratic formats of different speech corpora, while mirroring the structure of different data types improves speed and scalability. A Python API and a GUI both allow for: enriching the database with positional, hierarchical, temporal, and signal measures (eg utterance boundaries, f0) that are useful for linguistic analysis; querying the database using a simple query language; and exporting query results to standard formats for further analysis. We describe the software, summarize two case studies using it to examine effects on pitch and duration across languages, and outline planned future development.","tags":null,"title":"Polyglot and Speech Corpus Tools: A System for Representing, Integrating, and Querying Speech Corpora.","type":"publication"}]